{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark DataFrame Basic Handling\n",
    "- In comparison with RDD, pyspark DF is more structured, therefore supported with more functions such as sql like functions\n",
    "- its running is automatically optimized by internal optimizer called \"Catalyst Optimizer\"\n",
    "- to see the execution plan, hit pysparkDF.explain() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark-dataframe-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.219.115:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>taxi-analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa4714570a0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext, SparkSession\n",
    "- pyspark DF를 생성하기 위해 SparkSession 객체를 먼저 생성해야 한다\n",
    "- SparkContext 객체는 rdd 를 만들 수 있다.\n",
    "- SparkSession 객체 안에 SparkContext 객체가 있어 기본적으로 먼저 SparkSession 객체를 생성하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.219.115:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>taxi-analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=taxi-analysis>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. pyspark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_files = \"/Users/krafton/project/personal/spark-airflow-hands-on/data/trips/*\"\n",
    "zone_file = \"/Users/krafton/project/personal/spark-airflow-hands-on/data/taxi+_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f\"file:///{trip_files}\", inferSchema=True, header=True)\n",
    "zone_df = spark.read.csv(f\"file:///{zone_file}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------------------+------------+\n",
      "|LocationID|Borough      |Zone                   |service_zone|\n",
      "+----------+-------------+-----------------------+------------+\n",
      "|1         |EWR          |Newark Airport         |EWR         |\n",
      "|2         |Queens       |Jamaica Bay            |Boro Zone   |\n",
      "|3         |Bronx        |Allerton/Pelham Gardens|Boro Zone   |\n",
      "|4         |Manhattan    |Alphabet City          |Yellow Zone |\n",
      "|5         |Staten Island|Arden Heights          |Boro Zone   |\n",
      "+----------+-------------+-----------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|2       |2021-03-01 00:22:02 |2021-03-01 00:23:22  |1              |0.0          |1         |N                 |264         |264         |2           |3.0        |0.5  |0.5    |0.0       |0.0         |0.3                  |4.3         |0.0                 |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trips_df.join(zone_df.select(F.col(\"LocationID\").alias(\"PULocationID\"), F.col(\"Zone\").alias(\"pu_zone\")), \\\n",
    "                    \"PULocationID\", \"left_outer\")\\\n",
    "        .join(zone_df.select(F.col(\"LocationId\").alias(\"DOLocationID\"), F.col(\"Zone\").alias(\"do_zone\")), \\\n",
    "            \"DOLocationID\", \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+---------------------+------------+-------+-------+\n",
      "|DOLocationID|PULocationID|tpep_pickup_datetime|tpep_dropoff_datetime|total_amount|pu_zone|do_zone|\n",
      "+------------+------------+--------------------+---------------------+------------+-------+-------+\n",
      "|264         |264         |2021-03-01 00:22:02 |2021-03-01 00:23:22  |4.3         |NV     |NV     |\n",
      "+------------+------------+--------------------+---------------------+------------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------------+--------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|pu_zone              |do_zone       |total_amount|\n",
      "+--------------------+---------------------+---------------------+--------------+------------+\n",
      "|2021-03-01 00:07:40 |2021-03-01 00:31:23  |LaGuardia Airport    |NA            |70.07       |\n",
      "|2021-03-01 00:02:13 |2021-03-01 00:06:01  |East Chelsea         |NV            |11.16       |\n",
      "|2021-03-01 00:40:16 |2021-03-01 00:50:23  |Upper West Side South|Yorkville East|18.59       |\n",
      "+--------------------+---------------------+---------------------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.where(\"pu_zone != do_zone\")\\\n",
    "    .select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"pu_zone\", \"do_zone\", \"total_amount\")\\\n",
    "    .show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- pu_zone: string (nullable = true)\n",
      " |-- do_zone: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.where(\"pu_zone != do_zone\")\\\n",
    "    .select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"pu_zone\", \"do_zone\", \"total_amount\")\\\n",
    "    .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.cache()\n",
    "res.createOrReplaceTempView(\"res\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------------+--------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|pu_zone              |do_zone       |total_amount|\n",
      "+--------------------+---------------------+---------------------+--------------+------------+\n",
      "|2021-03-01 00:07:40 |2021-03-01 00:31:23  |LaGuardia Airport    |NA            |70.07       |\n",
      "|2021-03-01 00:02:13 |2021-03-01 00:06:01  |East Chelsea         |NV            |11.16       |\n",
      "|2021-03-01 00:40:16 |2021-03-01 00:50:23  |Upper West Side South|Yorkville East|18.59       |\n",
      "+--------------------+---------------------+---------------------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    tpep_pickup_datetime,\n",
    "    tpep_dropoff_datetime,\n",
    "    pu_zone,\n",
    "    do_zone,\n",
    "    total_amount\n",
    "from res\n",
    "where pu_zone != do_zone\n",
    "\"\"\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DOLocationID: int, PULocationID: int, VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: int, trip_distance: double, RatecodeID: int, store_and_fwd_flag: string, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, pu_zone: string, do_zone: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. execution plan of pyspark DF\n",
    "- Parsed Logical Plan -> Analyzed Logical Plan -> Optimized Logical Plan -> Physical Plan \n",
    "- Analyzed Logical Plan 에서 join 후 filter 하는 과정이 -> Optimized Logical Plan 에서 filter 하고 join 하는 것으로 변경\n",
    "- Physical Plan 에서 join 이 구체적으로 BroadcastHashJoin 으로 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|payment_type|   count|\n",
      "+------------+--------+\n",
      "|           1|10716903|\n",
      "|           3|   81434|\n",
      "|           4|   59664|\n",
      "|           2| 3308670|\n",
      "|        null|  834028|\n",
      "|           5|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df.groupBy(\"payment_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['trips DESC NULLS LAST], true\n",
      "+- 'Aggregate ['zone_data.Zone], ['zone_data.Zone, 'count(1) AS trips#2563]\n",
      "   +- 'Filter ('trip_data.payment_type = 1)\n",
      "      +- 'Join Inner, ('trip_data.PULocationID = 'zone_data.LocationID)\n",
      "         :- 'UnresolvedRelation [trip_data], [], false\n",
      "         +- 'UnresolvedRelation [zone_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Zone: string, trips: bigint\n",
      "Sort [trips#2563L DESC NULLS LAST], true\n",
      "+- Aggregate [Zone#2517], [Zone#2517, count(1) AS trips#2563L]\n",
      "   +- Filter (payment_type#2472 = 1)\n",
      "      +- Join Inner, (PULocationID#2470 = LocationID#2515)\n",
      "         :- SubqueryAlias trip_data\n",
      "         :  +- View (`trip_data`, [VendorID#2463,tpep_pickup_datetime#2464,tpep_dropoff_datetime#2465,passenger_count#2466,trip_distance#2467,RatecodeID#2468,store_and_fwd_flag#2469,PULocationID#2470,DOLocationID#2471,payment_type#2472,fare_amount#2473,extra#2474,mta_tax#2475,tip_amount#2476,tolls_amount#2477,improvement_surcharge#2478,total_amount#2479,congestion_surcharge#2480])\n",
      "         :     +- Relation [VendorID#2463,tpep_pickup_datetime#2464,tpep_dropoff_datetime#2465,passenger_count#2466,trip_distance#2467,RatecodeID#2468,store_and_fwd_flag#2469,PULocationID#2470,DOLocationID#2471,payment_type#2472,fare_amount#2473,extra#2474,mta_tax#2475,tip_amount#2476,tolls_amount#2477,improvement_surcharge#2478,total_amount#2479,congestion_surcharge#2480] csv\n",
      "         +- SubqueryAlias zone_data\n",
      "            +- View (`zone_data`, [LocationID#2515,Borough#2516,Zone#2517,service_zone#2518])\n",
      "               +- Relation [LocationID#2515,Borough#2516,Zone#2517,service_zone#2518] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [trips#2563L DESC NULLS LAST], true\n",
      "+- Aggregate [Zone#2517], [Zone#2517, count(1) AS trips#2563L]\n",
      "   +- Project [Zone#2517]\n",
      "      +- Join Inner, (PULocationID#2470 = LocationID#2515)\n",
      "         :- Project [PULocationID#2470]\n",
      "         :  +- Filter ((isnotnull(payment_type#2472) AND (payment_type#2472 = 1)) AND isnotnull(PULocationID#2470))\n",
      "         :     +- Relation [VendorID#2463,tpep_pickup_datetime#2464,tpep_dropoff_datetime#2465,passenger_count#2466,trip_distance#2467,RatecodeID#2468,store_and_fwd_flag#2469,PULocationID#2470,DOLocationID#2471,payment_type#2472,fare_amount#2473,extra#2474,mta_tax#2475,tip_amount#2476,tolls_amount#2477,improvement_surcharge#2478,total_amount#2479,congestion_surcharge#2480] csv\n",
      "         +- Project [LocationID#2515, Zone#2517]\n",
      "            +- Filter isnotnull(LocationID#2515)\n",
      "               +- Relation [LocationID#2515,Borough#2516,Zone#2517,service_zone#2518] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [trips#2563L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(trips#2563L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#3143]\n",
      "      +- HashAggregate(keys=[Zone#2517], functions=[count(1)], output=[Zone#2517, trips#2563L])\n",
      "         +- Exchange hashpartitioning(Zone#2517, 200), ENSURE_REQUIREMENTS, [id=#3140]\n",
      "            +- HashAggregate(keys=[Zone#2517], functions=[partial_count(1)], output=[Zone#2517, count#2568L])\n",
      "               +- Project [Zone#2517]\n",
      "                  +- BroadcastHashJoin [PULocationID#2470], [LocationID#2515], Inner, BuildRight, false\n",
      "                     :- Project [PULocationID#2470]\n",
      "                     :  +- Filter ((isnotnull(payment_type#2472) AND (payment_type#2472 = 1)) AND isnotnull(PULocationID#2470))\n",
      "                     :     +- FileScan csv [PULocationID#2470,payment_type#2472] Batched: false, DataFilters: [isnotnull(payment_type#2472), (payment_type#2472 = 1), isnotnull(PULocationID#2470)], Format: CSV, Location: InMemoryFileIndex(7 paths)[file:/Users/krafton/project/personal/spark-airflow-hands-on/data/trips..., PartitionFilters: [], PushedFilters: [IsNotNull(payment_type), EqualTo(payment_type,1), IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,payment_type:int>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#3135]\n",
      "                        +- Filter isnotnull(LocationID#2515)\n",
      "                           +- FileScan csv [LocationID#2515,Zone#2517] Batched: false, DataFilters: [isnotnull(LocationID#2515)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/krafton/project/personal/spark-airflow-hands-on/data/taxi+..., PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int,Zone:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.createOrReplaceTempView(\"trip_data\")\n",
    "zone_df.createOrReplaceTempView(\"zone_data\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT zone_data.Zone, count(*) AS trips\n",
    "FROM trip_data JOIN zone_data \n",
    "    ON trip_data.PULocationID = zone_data.LocationID\n",
    "WHERE trip_data.payment_type=1\n",
    "GROUP BY zone_data.Zone\n",
    "ORDER BY trips DESC\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3269cb3f0fcdc29040b68ea524d0bc37f6c9803c45e4e744e9ef495742d68083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spark-airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
