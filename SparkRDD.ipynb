{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRAFTONui-MacBookPro.local\n",
      "##\n",
      "# Host Database\n",
      "#\n",
      "# localhost is used to configure the loopback interface\n",
      "# when the system is booting.  Do not change this entry.\n",
      "##\n",
      "127.0.0.1\tlocalhost\n",
      "127.0.0.1 KRAFTONui-MacBookPro.local\n",
      "255.255.255.255\tbroadcasthost\n",
      "::1             localhost\n",
      "# Added by Docker Desktop\n",
      "# To allow the same kube context to work on the host and the container:\n",
      "127.0.0.1 kubernetes.docker.internal\n",
      "# End of section\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!cat /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/11 12:45:22 WARN Utils: Your hostname, KRAFTONui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.17 instead (on interface en0)\n",
      "22/03/11 12:45:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/11 12:45:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"rdd-test\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.17:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rdd-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=rdd-test>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_files = \"/Users/krafton/project/personal/spark-airflow-hands-on/data/trips/yellow_tripdata_2021-01.csv\"\n",
    "zone_file = \"/Users/krafton/project/personal/spark-airflow-hands-on/data/taxi+_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(f\"file:///{trip_files}\")\n",
    "header = lines.first()\n",
    "filtered_lines = lines.filter(lambda row:row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'str'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lines))\n",
    "print(type(header))\n",
    "print(type(filtered_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"짜장면\", \"마라탕\", \"짬뽕\", \"떡볶이\", \"쌀국수\", \"짬뽕\", \"짜장면\", \"짜장면\", \"짜장면\",  \"라면\", \"우동\", \"라면\"])\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '짬뽕', '짜장면', '짜장면', '짜장면', '라면', '우동', '라면']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Basic Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'짜장면': 4,\n",
       "             '마라탕': 1,\n",
       "             '짬뽕': 2,\n",
       "             '떡볶이': 1,\n",
       "             '쌀국수': 1,\n",
       "             '라면': 2,\n",
       "             '우동': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '라면', '우동']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면tt', '마라탕tt', '짬뽕tt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x+'tt').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['마라탕', '짬뽕', '떡볶이']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: x != '짜장면').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜', '면', '마라탕']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split('장')).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'짜장면마라탕짬뽕떡볶이쌀국수짬뽕짜장면짜장면짜장면라면우동라면'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_groupby = rdd.groupBy(lambda x: x[0])\n",
    "type(rdd_groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spark-airflow/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('짜', <pyspark.resultiterable.ResultIterable at 0x7fb6f80792e0>),\n",
       " ('마', <pyspark.resultiterable.ResultIterable at 0x7fb6f8079790>),\n",
       " ('짬', <pyspark.resultiterable.ResultIterable at 0x7fb6f8079310>)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_groupby.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "짜 ['짜장면', '짜장면', '짜장면', '짜장면']\n",
      "마 ['마라탕']\n",
      "짬 ['짬뽕', '짬뽕']\n",
      "떡 ['떡볶이']\n",
      "쌀 ['쌀국수']\n",
      "라 ['라면', '라면']\n",
      "우 ['우동']\n"
     ]
    }
   ],
   "source": [
    "for k, v in rdd_groupby.collect():\n",
    "    print(k, list(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key-value RDD\n",
    "- pair RDD 로 기존의 RDD와 크게 다르지 않다\n",
    "- key-value 형태로 데이터프레임처럼 사용할 수 있다는 장점\n",
    "- key-value 로 나뉘어져 .mapValues(), .reduceByKey() 등을 사용할 수 있음\n",
    "- value를 다루는 경우 .map() 보다는 .mapValues() 를 사용하자\n",
    "  - 내부적으로 .mapValues()를 사용하면 파티션을 그대로 유지하면서 values 만을 건드려서 효율적\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 1),\n",
       " ('마라탕', 1),\n",
       " ('짬뽕', 1),\n",
       " ('떡볶이', 1),\n",
       " ('쌀국수', 1),\n",
       " ('짬뽕', 1),\n",
       " ('짜장면', 1),\n",
       " ('짜장면', 1),\n",
       " ('짜장면', 1),\n",
       " ('라면', 1),\n",
       " ('우동', 1),\n",
       " ('라면', 1)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rdd.map(lambda x: (x, 1))\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '짬뽕', '짜장면', '짜장면', '짜장면', '라면', '우동', '라면']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(res.keys().collect())\n",
    "print(res.values().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 11),\n",
       " ('마라탕', 11),\n",
       " ('짬뽕', 11),\n",
       " ('떡볶이', 11),\n",
       " ('쌀국수', 11),\n",
       " ('짬뽕', 11),\n",
       " ('짜장면', 11),\n",
       " ('짜장면', 11),\n",
       " ('짜장면', 11),\n",
       " ('라면', 11),\n",
       " ('우동', 11),\n",
       " ('라면', 11)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.mapValues(lambda x: x+10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 4),\n",
       " ('마라탕', 1),\n",
       " ('짬뽕', 2),\n",
       " ('떡볶이', 1),\n",
       " ('쌀국수', 1),\n",
       " ('라면', 2),\n",
       " ('우동', 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "짜장면\n",
      "마라탕\n",
      "짬뽕\n",
      "떡볶이\n",
      "쌀국수\n",
      "짬뽕\n",
      "짜장면\n",
      "짜장면\n",
      "짜장면\n",
      "라면\n",
      "우동\n",
      "라면\n"
     ]
    }
   ],
   "source": [
    "### foreach 는 rdd element 마다 해당 element 를 가지고 있는 executor 단에서 실행됨\n",
    "### 여기서는 local에서 master 와 executor 를 둘다 가지고 있어 print 구문이 보이지만,\n",
    "### 분산 환경에서 실행하면 print 구문이 보이지 않는다 (executor node 에 들어가야 보임)\n",
    "### 이를 활용하여 executor node 에 pip install 을 보내는 방식으로 사용할 수도 있다\n",
    "rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map, reduce 기능\n",
    "- Spark는 분산된 환경에서 데이터 병렬 모델을 구현해 추상화 시켜준다\n",
    "- map - reduce 의 병렬처리 질의문을 분산 환경에서도 일반적인 병렬 처리를 하듯이 추상화시켜준 것이다\n",
    "  - 노드간 통신 같은 분산 환경에서 고려해야 하는 부분들은 스파크 내부 단에서 자동으로 처리해줌\n",
    "- 그렇지만 성능 개선을 위해 분산 환경에서 병렬 처리하는 뒷단의 과정을 고려하여 코딩을 해야 한다\n",
    "  - wide transformation (partition을 재조합 하는 과정) 이 비싼 과정임을 염두하자\n",
    "  - .reduceByKey() 이전에 .filter()를 해주어 네트워크 통신 비용을 줄이도록 하자\n",
    "- RDD 에서 고려해야 하는 이러한 성능 최적화가 DataFrame 에서는 내부 엔진 (Catalyst Optimizer)에서 자동으로 해준다\n",
    "  - RDD는 row level 이다. 학습을 위해 그리고 정말 특수한 경우의 최적화를 위해 사용하고,\n",
    "  - 가능하면 DF를 사용하자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) which one is better?\n",
    "- paired_rdd.map().reduceByKey().filter().take(100)\n",
    "- paired_rdd.map().filter().reduceByKey().take(100)\n",
    "\n",
    "### 2) Answer\n",
    "- reduceByKey() 는 shuffling 하는 함수이다. 즉, 비용이 비싼 함수!\n",
    "- shuffling 하기 전에 filter() 를 해서 데이터를 줄여 네트워크 통신 비용을 줄이는 것이 성능 개선에 유리\n",
    "\n",
    "### 3) narrow - wide transformation\n",
    "- narrow: filter(), map(), flatMap(), sample(), union()\n",
    "- wide: shuffling, join, reduceByKey(), groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '짬뽕', '짜장면', '짜장면', '짜장면', '라면', '우동', '라면']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spark-airflow/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('마라탕', 1), ('짬뽕', 2), ('떡볶이', 1), ('쌀국수', 1), ('라면', 2), ('우동', 1)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y).filter(lambda x: x[0]!='짜장면').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('마라탕', 1), ('짬뽕', 2), ('떡볶이', 1), ('쌀국수', 1), ('라면', 2), ('우동', 1)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (x, 1)).filter(lambda x: x[0]!='짜장면').reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파티셔닝 기능\n",
    "- 노드에서 locally 연산을 최대한 수행하게끔 파티셔닝을 효율적으로 하면 성능개선에 유리함\n",
    "- 분산 환경에서 데이터를 최대한 균일하게 퍼트려서 부하를 조절하는 것도 파티셔닝의 역할\n",
    "- shuffling, 파티셔닝을 재조합하는 연산 이후에는 caching 하여 해당 연산이 반복되지 않도록 하자!\n",
    "  - join, reduceByKey 같은 연산이 shuffling을 일으키며 비용이 비싼 연산이다.\n",
    "  - 한번만 쓰는 경우라면 굳이 caching 하지 않는게 유리 -> 캐싱하는데에 시간이 걸림\n",
    "  - 반복적으로 사용한다면 꼭 캐싱하자\n",
    "- 파티셔닝 종류\n",
    "  - Hash Partitioning\n",
    "  - Range Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 2), (3, 3), (4, 4), (2, 2), (4, 4), (1, 1)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 2), (4, 4), (2, 2), (4, 4)], [(1, 1), (3, 3), (1, 1)]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.partitionBy(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 2), (4, 4), (2, 2), (4, 4)], [(1, 1), (3, 3), (1, 1)]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.partitionBy(2, lambda x: x%2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cache - persist\n",
    "- 스파크는 lazy-evaluatoin 이다\n",
    "  - transformation 을 논리적으로 가지고 있다가\n",
    "  - action 을 할 때에 그제서야 데이터를 memory로 옮기고 연산을 수행하여 결과를 낸다\n",
    "  - 그리고 나서 memory 에 저장된 데이터를 다시 풀어준다\n",
    "- 반복적으로 연산을 수행하는 데이터의 경우 memory 에 캐싱하여 사용하는 것이 효율적이다\n",
    "  - .cache()\n",
    "  - 메모리 할당을 풀어주려면 -> .unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[7] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '짬뽕', '짜장면', '짜장면', '짜장면', '라면', '우동', '라면']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[7] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '마라탕', '짬뽕', '떡볶이', '쌀국수', '짬뽕', '짜장면', '짜장면', '짜장면', '라면', '우동', '라면']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3269cb3f0fcdc29040b68ea524d0bc37f6c9803c45e4e744e9ef495742d68083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spark-airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
