{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/11 14:06:17 WARN Utils: Your hostname, KRAFTONui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.17 instead (on interface en0)\n",
      "22/03/11 14:06:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/11 14:06:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/11 14:06:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY=\"5g\"\n",
    "spark = SparkSession.builder.appName(\"SparkML-test\")\\\n",
    "                .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_files = \"/Users/krafton/project/personal/spark-airflow-hands-on/data/trips/*\"\n",
    "zone_file = \"/Users/krafton/project/personal/spark-airflow-hands-on/data/taxi+_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv(f\"file:///{trip_files}\", inferSchema=True, header=True)\n",
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 4\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+-----------+------------------+\n",
      "|summary|   passenger_count|pickup_location_id|dropoff_location_id|     trip_distance|       pickup_time|day_of_week|      total_amount|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+-----------+------------------+\n",
      "|  count|          13126040|          13126040|           13126040|          13126040|          13126040|   13126040|          13126040|\n",
      "|   mean|1.2114008489993935|166.21161279411004| 163.70421993228726|2.8820930920520915|14.206110144415224|       null|17.973158757890285|\n",
      "| stddev|0.5424025151958398| 65.92863093005907|  70.62329603504969| 3.820306480671185| 5.118095829304276|       null|12.975904680786682|\n",
      "|    min|                 0|                 1|                  1|              0.01|                 0|     Friday|              0.01|\n",
      "|    max|                 3|               265|                265|             475.5|                23|  Wednesday|            4973.3|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")\n",
    "data_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly Data Split for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/krafton/project/personal/spark-airflow-hands-on/data/model_data/'\n",
    "\n",
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering\n",
    "- pre-processing\n",
    "  - categorical attribution -> one-hot encoding\n",
    "  - numerical attribution -> scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "cat_feats = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "stages = []\n",
    "\n",
    "for c in cat_feats:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_feats = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_feats:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol= n + \"_vector\")\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol= n + \"_scaled\")\n",
    "    stages += [num_assembler, num_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_076fc829c6ab,\n",
       " OneHotEncoder_37910b92b78a,\n",
       " StringIndexer_6226db406e91,\n",
       " OneHotEncoder_d4f43edcfa36,\n",
       " StringIndexer_c824080a8cc0,\n",
       " OneHotEncoder_32fcbb34a904,\n",
       " VectorAssembler_954291f2e685,\n",
       " StandardScaler_3b989430f56c,\n",
       " VectorAssembler_b9a359db1458,\n",
       " StandardScaler_71cc879e9f5f,\n",
       " VectorAssembler_a968cd8516f3,\n",
       " StandardScaler_34fa9f97d180,\n",
       " VectorAssembler_a1bb683d1332]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages=transform_stages)\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrain_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+------------------+\n",
      "|day_of_week|day_of_week_idx|day_of_week_onehot|\n",
      "+-----------+---------------+------------------+\n",
      "|Saturday   |4.0            |(7,[4],[1.0])     |\n",
      "|Friday     |0.0            |(7,[0],[1.0])     |\n",
      "|Thursday   |1.0            |(7,[1],[1.0])     |\n",
      "|Sunday     |6.0            |(7,[6],[1.0])     |\n",
      "|Wednesday  |2.0            |(7,[2],[1.0])     |\n",
      "|Tuesday    |3.0            |(7,[3],[1.0])     |\n",
      "|Monday     |5.0            |(7,[5],[1.0])     |\n",
      "+-----------+---------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vtrain_df.select(\"day_of_week\", \"day_of_week_idx\", \"day_of_week_onehot\")\\\n",
    "    .distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+----------------------+\n",
      "|passenger_count|passenger_count_vector|passenger_count_scaled|\n",
      "+---------------+----------------------+----------------------+\n",
      "|2              |[2.0]                 |[3.6871269716505233]  |\n",
      "|0              |[0.0]                 |[0.0]                 |\n",
      "|1              |[1.0]                 |[1.8435634858252616]  |\n",
      "|3              |[3.0]                 |[5.530690457475785]   |\n",
      "+---------------+----------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vtrain_df.select(\"passenger_count\", \"passenger_count_vector\", \"passenger_count_scaled\")\\\n",
    "    .distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|feature_vector                                                                  |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|(533,[47,264,528,531,532],[1.0,1.0,1.0,1.8335617216255875,3.7124963951492775])  |\n",
      "|(533,[68,330,527,531,532],[1.0,1.0,1.0,0.9691683385735249,4.4940745836017575])  |\n",
      "|(533,[44,296,528,531,532],[1.0,1.0,1.0,0.23574364992328983,3.321707300923038])  |\n",
      "|(533,[24,302,527,531,532],[1.0,1.0,1.0,0.7596184275306005,2.930918206696798])   |\n",
      "|(533,[24,271,524,531,532],[1.0,1.0,1.0,0.1571624332821932,2.1493400182443185])  |\n",
      "|(533,[60,289,527,531,532],[1.0,1.0,1.0,0.7858121664109661,0.39078909422623975]) |\n",
      "|(533,[11,273,527,531,532],[1.0,1.0,1.0,0.05238747776073108,0.39078909422623975])|\n",
      "|(533,[11,285,523,531,532],[1.0,1.0,1.0,0.4190998220858486,2.3447345653574385])  |\n",
      "|(533,[37,274,527,531,532],[1.0,1.0,1.0,0.6286497331287728,3.7124963951492775])  |\n",
      "|(533,[19,270,527,531,532],[1.0,1.0,1.0,0.681037210889504,4.298680036488637])    |\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vtrain_df.select(\"feature_vector\")\\\n",
    "    .distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LinearRegression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/11 15:18:33 WARN Instrumentation: [52518dd3] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/03/11 15:18:46 WARN Instrumentation: [52518dd3] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "22/03/11 15:18:46 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/03/11 15:18:46 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8084901250081301"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.648520165266761"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtest_df = fitted_transformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, feature_vector: vector, prediction: double]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.transform(vtest_df)\n",
    "prediction.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          1.6|   Thursday|        12.3|15.150622157886815|\n",
      "|          3.3|   Saturday|       23.15|20.815012529756736|\n",
      "|          4.1|  Wednesday|        16.3|  15.2287766581555|\n",
      "|          0.4|   Thursday|         5.8|  7.50328020386138|\n",
      "|         15.4|   Thursday|        65.3| 46.03403283496961|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select([\"trip_distance\", \"day_of_week\", \"total_amount\", \"prediction\"])\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    maxIter=30,\n",
    "    solver=\"normal\",\n",
    "    labelCol='total_amount',\n",
    "    featuresCol='feature_vector'\n",
    ")\n",
    "\n",
    "cv_stages = stages + [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pipeline = Pipeline(stages=cv_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder()\\\n",
    "                .addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3, 0.4, 0.5])\\\n",
    "                .addGrid(lr.regParam, [0.01, 0.02, 0.03, 0.04, 0.05])\\\n",
    "                .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = CrossValidator(estimator=cv_pipeline,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=RegressionEvaluator(labelCol=\"total_amount\"),\n",
    "                           numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "toy_df = train_df.sample(False, 0.1, seed=1)\n",
    "cv_model = cross_val.fit(toy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam()\n",
    "reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()\n",
    "print(alpha)\n",
    "print(reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 최종 모델\n",
    "- hyperparameter 튜닝 후 적용\n",
    "- 모델 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8084602992197699"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver=\"normal\",\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\",\n",
    "    elasticNetParam=alpha,\n",
    "    regParam=reg_param,\n",
    ")\n",
    "\n",
    "model = lr.fit(vtrain_df)\n",
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/Users/krafton/project/personal/spark-airflow-hands-on/data/model/'\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "lr_model = LinearRegressionModel().load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_a0df54742b30, numFeatures=533"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3269cb3f0fcdc29040b68ea524d0bc37f6c9803c45e4e744e9ef495742d68083"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spark-airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
